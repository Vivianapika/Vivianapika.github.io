---
title: "Learning spaCy"
author: "Vivian"
date: "2024-06-12"
categories: [nlp, code]
image: "image.jpg"
jupyter: env
code-fold: true
---

[Spacy 101](https://spacy.io/usage/spacy-101)

## Pipeline and Architecture
![pipeline](pipeline.svg)
![Architecture](architecture.svg){width=70%}

## Vocab, Lexemes and StringStore
```{python}
import spacy
nlp = spacy.load("en_core_web_sm")

# Shared vocab and string store
nlp.vocab.strings.add("coffee")
coffee_hash = nlp.vocab.strings["coffee"]
coffee_string = nlp.vocab.strings[coffee_hash]
doc = nlp("I love coffee")
print("hash value:", nlp.vocab.strings["coffee"])
print("string value:", nlp.vocab.strings[3197928453018144401])
```

```{python}
# Lexemes: entries in the vocabulary
doc = nlp("I love coffee")
lexeme = nlp.vocab["coffee"]
# Print the lexical attributes
print(lexeme.text, lexeme.orth, lexeme.is_alpha)
```

## Doc, Span and Token
```{python}
# Create an nlp object
nlp = spacy.blank("en")

# Import the Doc and Span classes
from spacy.tokens import Doc, Span

# The words and spaces to create the doc from
words = ["I", "like", "David", "Bowie"]
spaces = [True, True, True, False]

# Create a doc manually
doc = Doc(nlp.vocab, words=words, spaces=spaces)
print(doc.text)

# Create a span manually
# span = Span(doc, 0, 2)

# Create a span with a label
# Create a span for "David Bowie" from the doc and assign it the label "PERSON"
span_with_label = Span(doc, 2, 4, label="PERSON")
print(span_with_label.text, span_with_label.label_)

# Add span to the doc.ents (doc's entities)
doc.ents = [span_with_label]

# Print entities' text and labels
print([(ent.text, ent.label_) for ent in doc.ents])
```

## Data structures best practices
```{python}
nlp = spacy.load("en_core_web_sm")
doc = nlp("Berlin looks like a nice city")

for token in doc:
    # Check if the current token is a proper noun
    if token.pos_ == "PROPN":
        # Check if the next token is a verb
        if token.i + 1 < len(doc):
            if doc[token.i + 1].pos_ == "VERB":
                result = token.text
                print("Found proper noun before a verb:", result)
```

## Word vectors and semantic similarity
```{python}
# Load a larger pipeline with vectors
nlp = spacy.load("en_core_web_md")

# Compare two documents
doc1 = nlp("I like fast food")
doc2 = nlp("I like pizza")
print(doc1.similarity(doc2))

# Compare two tokens
doc = nlp("I like pizza and pasta")
token1 = doc[2]
token2 = doc[4]
print(token1.similarity(token2))
```


```{python}
# Load a larger pipeline with vectors
nlp = spacy.load("en_core_web_md")

doc = nlp("I have a banana")
# Access the vector via the token.vector attribute
print(doc[3].vector)
```

## Combining predictions and rules
```{python}
# Initialize with the shared vocab
from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)

# Patterns are lists of dictionaries describing the tokens
pattern = [{"LEMMA": "love", "POS": "VERB"}, {"LOWER": "cats"}]
matcher.add("LOVE_CATS", [pattern])

# Operators can specify how often a token should be matched
pattern = [{"TEXT": "very", "OP": "+"}, {"TEXT": "happy"}]
matcher.add("VERY_HAPPY", [pattern])

# Calling matcher on doc returns list of (match_id, start, end) tuples
doc = nlp("I love cats and I'm very very happy")
matches = matcher(doc)
```


PhraseMatcher like regular expressions or keyword search â€“ but with access to the tokens!
```{python}
from spacy.matcher import PhraseMatcher

matcher = PhraseMatcher(nlp.vocab)

pattern = nlp("Golden Retriever")
matcher.add("DOG", [pattern])
doc = nlp("I have a Golden Retriever")

# Iterate over the matches
for match_id, start, end in matcher(doc):
    # Get the matched span
    span = doc[start:end]
    print("Matched span:", span.text)
```